{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HcV0pLdZrHL3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir('./drive/MyDrive/signature_data_one_shot/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "g49ti9_kWwHh"
      },
      "outputs": [],
      "source": [
        "def get_data(dir):\n",
        "    orig = []\n",
        "    forg = []\n",
        "    for name in tqdm(sorted(os.listdir(dir))):\n",
        "        original = []\n",
        "        forged = []\n",
        "        for image_name in sorted(os.listdir(dir+'/'+name)):\n",
        "            img = dir+'/'+name+'/'+image_name\n",
        "            if 'forg' in name.lower():\n",
        "                forged.append(img)\n",
        "            else:\n",
        "                original.append(img)\n",
        "        \n",
        "        if 'forg' in name.lower():\n",
        "            forg.append(forged)\n",
        "        else:\n",
        "            orig.append(original)\n",
        "    \n",
        "    return orig, forg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IftMDRuj9JAP"
      },
      "outputs": [],
      "source": [
        "train_orig, train_forg = get_data('train_data')\n",
        "validation_orig, validation_forg = get_data('validation_data')\n",
        "test_orig, test_forg = get_data('test_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8vYjr9rnNGD"
      },
      "outputs": [],
      "source": [
        "print(len(train_orig))\n",
        "print(len(train_forg))\n",
        "print(len(validation_orig))\n",
        "print(len(validation_forg))\n",
        "print(len(test_orig))\n",
        "print(len(test_forg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBKhhhmAojE0"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vects):\n",
        "    '''Compute Euclidean Distance between two vectors'''\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "     \n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_batch(orig_groups, forg_groups, batch_size = 32):\n",
        "    '''Function to generate a batch of data with batch_size number of data points\n",
        "    Half of the data points will be Genuine-Genuine pairs and half will be Genuine-Forged pairs'''\n",
        "    while True:\n",
        "        orig_pairs = []\n",
        "        forg_pairs = []\n",
        "        gen_gen_labels = []\n",
        "        gen_for_labels = []\n",
        "        all_pairs = []\n",
        "        all_labels = []\n",
        "        \n",
        "        for orig, forg in zip(orig_groups, forg_groups):\n",
        "            orig_pairs.extend(list(itertools.combinations(orig, 2)))\n",
        "            for i in range(len(orig)):\n",
        "                forg_pairs.extend(list(itertools.product(orig[i:i+1], forg)))\n",
        "  \n",
        "        gen_gen_labels = [1]*len(orig_pairs)\n",
        "        gen_for_labels = [0]*len(forg_pairs)\n",
        "        \n",
        "        all_pairs = orig_pairs + forg_pairs\n",
        "        all_labels = gen_gen_labels + gen_for_labels\n",
        "        del orig_pairs, forg_pairs, gen_gen_labels, gen_for_labels\n",
        "        all_pairs, all_labels = shuffle(all_pairs, all_labels)\n",
        "            \n",
        "        k = 0\n",
        "        pairs=[np.zeros((batch_size, 268,650,1)) for i in range(2)]\n",
        "        targets=np.zeros((batch_size,))\n",
        "        for ix, pair in enumerate(all_pairs):\n",
        "            img1 = cv2.imread(pair[0], cv2.IMREAD_GRAYSCALE)\n",
        "            img2 = cv2.imread(pair[1], cv2.IMREAD_GRAYSCALE)\n",
        "            img1 = np.array(img1, dtype = np.float64)\n",
        "            img2 = np.array(img2, dtype = np.float64)\n",
        "            img1 /= 255\n",
        "            img2 /= 255\n",
        "            img1 = img1[..., np.newaxis]\n",
        "            img2 = img2[..., np.newaxis]\n",
        "            pairs[0][k, :, :, :] = img1\n",
        "            pairs[1][k, :, :, :] = img2\n",
        "            targets[k] = all_labels[ix]\n",
        "            k += 1\n",
        "            if k == batch_size:\n",
        "                yield pairs, targets\n",
        "                k = 0\n",
        "                pairs=[np.zeros((batch_size, 268,650,1)) for i in range(2)]\n",
        "                targets=np.zeros((batch_size,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_base_network_signal(input_shape):\n",
        "    '''Base Siamese Network'''\n",
        "    \n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape,padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2),padding='same'))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2),padding='same'))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2),padding='same'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1024, activation='relu'))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape=(268,650,1)\n",
        "base_network = create_base_network_signal(input_shape)\n",
        "\n",
        "input_a = layers.Input(shape=(input_shape))\n",
        "input_b = layers.Input(shape=(input_shape))\n",
        "\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = layers.Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "prediction = layers.Dense(1,activation='sigmoid')(distance)\n",
        "\n",
        "model = models.Model(inputs=[input_a, input_b], outputs=prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_train_samples = num_test_samples = 66 * 50 + 96 * 50\n",
        "num_val_samples = 66 * 7 + 96 * 7\n",
        "num_train_samples, num_test_samples, num_val_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "history = model.fit(\n",
        "    generate_batch(train_orig, train_forg, batch_size),\n",
        "                   steps_per_epoch = num_train_samples//batch_size,\n",
        "                   epochs = 4,\n",
        "                   validation_data = generate_batch(validation_orig, validation_forg, batch_size),\n",
        "                   validation_steps = num_val_samples//batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('siamese.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
